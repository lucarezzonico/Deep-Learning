{"cells":[{"cell_type":"markdown","metadata":{"id":"Gb03R-oINjO0"},"source":["## DL project colab notebook\n","\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"e8ZKW4STOlyI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/lucarezzonico/Deep_Learning"],"metadata":{"id":"rhJOVbvrwbwD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd Deep_Learning"],"metadata":{"id":"OlOpN4M3167c"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"H0c3K5CGNjO6","executionInfo":{"status":"ok","timestamp":1653300031979,"user_tz":-120,"elapsed":2795,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch import optim\n","\n","import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n","import matplotlib.pyplot as plt\n","\n","from Miniproject_1.model import Model\n","\n","# import torch.utils.data as utils\n","# from torchvision import datasets, transforms\n","# from torch.utils.data.sampler import SubsetRandomSampler\n","import matplotlib.pyplot as plt\n","# %matplotlib inline"]},{"cell_type":"markdown","source":["Load Data and show some images"],"metadata":{"id":"DB7CawqhW_Xx"}},{"cell_type":"code","source":["# Read the data from the file\n","noisy_imgs_train_1, noisy_imgs_train_2 = torch.load(open('/content/drive/MyDrive/Colab Notebooks/DL colabs/miniproject_dataset/train_data.pkl', 'rb'))\n","noisy_imgs_valid, clean_imgs_valid = torch.load(open('/content/drive/MyDrive/Colab Notebooks/DL colabs/miniproject_dataset/val_data.pkl', 'rb'))\n","\n","print('noisy_imgs_train_1', noisy_imgs_train_1.size(), 'noisy_imgs_train_2', noisy_imgs_train_2.size())\n","print('noisy_imgs_valid', noisy_imgs_valid.size(), 'clean_imgs_valid', clean_imgs_valid.size())\n","\n","def compute_psnr_mean(x, y):\n","    assert x.shape == y.shape and x.ndim == 4\n","    return - 10 * torch.log10(((x-y) ** 2).mean((1,2,3))).mean()\n","\n","def compute_psnr_std(x, y):\n","    assert x.shape == y.shape and x.ndim == 4\n","    return - 10 * torch.log10(((x-y) ** 2).mean((1,2,3))).std()\n","\n","def plot_images(*args, titles):\n","    for i in range(args[0].size(dim=0)): # number images to plot for each dataset\n","        if len(args) > 1: _, axes = plt.subplots(1, len(args))\n","        for img, idx in zip(args, range(len(args))): # number datasets to plot\n","            if len(args) > 1:\n","                axes[idx].imshow(img[i,:,:,:].permute((1, 2, 0)))\n","                axes[idx].set_title(titles[idx])\n","            else:\n","                plt.imshow(img[i, :, :, :].permute((1, 2, 0)))\n","                plt.title(titles[idx])\n","        plt.show()\n","\n","# plot_images(noisy_imgs_train_1[0:4, :, :, :], noisy_imgs_train_2[0:4, :, :, :], titles=['noisy_imgs_train_1','noisy_imgs_train_2'])\n","# plot_images(noisy_imgs_valid[0:4, :, :, :], clean_imgs_valid[0:4, :, :, :], titles=['noisy_imgs_valid','clean_imgs_valid'])\n","\n","# # transform data\n","# my_transforms = transforms.Compose(\n","#     [   # Compose makes it possible to have many transforms\n","#         # transforms.ToPILImage(),\n","#         transforms.Resize((36, 36)),  # Resizes (32,32) to (36,36)\n","#         transforms.RandomCrop((32, 32)),  # Takes a random (32,32) crop\n","#         transforms.ColorJitter(brightness=0.5),  # Change brightness of image\n","#         transforms.RandomRotation(degrees=45),  # Perhaps a random rotation from -45 to 45 degrees\n","#         transforms.RandomHorizontalFlip(p=0.5),  # Flips the image horizontally with probability 0.5\n","#         transforms.RandomVerticalFlip(p=0.05),  # Flips image vertically with probability 0.05\n","#         transforms.RandomGrayscale(p=0.2),  # Converts to grayscale with probability 0.2\n","#         # transforms.ToTensor(),  # Finally converts PIL image to tensor so we can train w. pytorch\n","#         # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Note: these values aren't optimal\n","#     ]\n","# )\n","#\n","# transformed_imgs = my_transforms(noisy_imgs_train_1[0:4, :, :, :])\n","# plot_images(transformed_imgs, titles=['transformed_imgs'])\n","\n","################################################################################\n","\n","#subset of data\n","train_data_upper_index = 1000\n","train_input = noisy_imgs_train_1[0:train_data_upper_index, :, :, :]\n","train_target = noisy_imgs_train_2[0:train_data_upper_index, :, :, :]\n","test_input = noisy_imgs_valid[0:train_data_upper_index, :, :, :]\n","test_target = clean_imgs_valid[0:train_data_upper_index, :, :, :]\n","\n","# model = Model(lr=1e-1, optimizer='SGD', criterion='MSE')\n","# model = Model(lr=1e-3, optimizer='Adam', criterion='MSE')\n","# model = Model(lr=1e-3, optimizer='Adagrad', criterion='MSE')\n","model = Model(lr=5e-1, optimizer='Adadelta', criterion='MSE')\n","\n","# train\n","model.train(train_input, train_target, num_epochs=15, mini_batch_size = 20)\n","model.save_model()\n","\n","# load model\n","model.load_pretrained_model()\n","\n","# denoise input\n","denoised_test_input = model.predict(test_input)\n","denoised_test_input = denoised_test_input.detach()\n","\n","# PSNR\n","psnr_mean = float(compute_psnr_mean(denoised_test_input, test_target.float().div(255)))\n","psnr_std = abs(float(compute_psnr_std(denoised_test_input, test_target.float().div(255))))\n","print('mean psnr = {:.2f}'.format(psnr_mean),'dB', 'std psnr = {:.2f}'.format(psnr_std),'dB')\n","\n","denoised_test_input = denoised_test_input.mul(255).to(torch.uint8)\n","\n","# plot denoised image\n","plot_images(test_input[0:4,:,:,:], denoised_test_input[0:4,:,:,:], test_target[0:4,:,:,:], titles=['test_input','denoised_test_input','test_target'])\n","\n","# mse or this loss?\n","# loss = 0.5 * (denoised_test_input - input).pow(2).sum() / input.size(0)\n"],"metadata":{"id":"kpSL2MzAOWnk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train"],"metadata":{"id":"I3GmULLa3eht"}},{"cell_type":"code","source":[""],"metadata":{"id":"2PtxXob63bb3","executionInfo":{"status":"aborted","timestamp":1653300142017,"user_tz":-120,"elapsed":13,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"0JpvOE0GfwgG"}},{"cell_type":"code","source":[""],"metadata":{"id":"QBINJe7-fiiz","executionInfo":{"status":"aborted","timestamp":1653300142019,"user_tz":-120,"elapsed":14,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting the loss history for different learning rates\n","plt.plot((np.arange(np.shape(loss_history_training)[0])+1)*print_every, loss_history_training, label='Training Loss')\n","plt.plot((np.arange(np.shape(loss_history_validation)[0])+1)*print_every, loss_history_validation, label='Validation Loss')\n","plt.xlabel(\"Iteration\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Training & Validation Loss\")\n","plt.legend(bbox_to_anchor = (1, 1))\n","plt.grid()\n","plt.show() # Load the display window"],"metadata":{"id":"ngepQ92u9Kcm","executionInfo":{"status":"aborted","timestamp":1653300142021,"user_tz":-120,"elapsed":16,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting the loss history for different learning rates\n","plt.plot((np.arange(np.shape(accuracy_history_training)[0])+1)*print_every, accuracy_history_training, label='Training Accuracy')\n","plt.plot((np.arange(np.shape(accuracy_history_validation)[0])+1)*print_every, accuracy_history_validation, label='Validation Accuracy')\n","plt.xlabel(\"Iteration\")\n","plt.ylabel(\"Accuracy [%]\")\n","plt.title(\"Training & Validation Accuracy\")\n","plt.legend(bbox_to_anchor = (1, 1))\n","plt.grid()\n","plt.show() # Load the display window"],"metadata":{"id":"oMwQGcwqj2Rl","executionInfo":{"status":"aborted","timestamp":1653300142023,"user_tz":-120,"elapsed":18,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zUqGBbcpNjPB"},"source":["Save best trained model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BKFS7ALNjPB","executionInfo":{"status":"aborted","timestamp":1653300142024,"user_tz":-120,"elapsed":19,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"}}},"outputs":[],"source":["## You should be familiar with how to save a pytorch model (Make sure to save the model in your Drive)\n","\n","torch.save(net.state_dict(), 'drive/MyDrive/Colab Notebooks/tp_3/ConvNN.ckpt')"]},{"cell_type":"code","source":["checkpoint = torch.load(\"drive/MyDrive/Colab Notebooks/tp_3/ConvNN.ckpt\")\n","net.load_state_dict(checkpoint)"],"metadata":{"id":"Is7LK6dhfVph","executionInfo":{"status":"aborted","timestamp":1653300142025,"user_tz":-120,"elapsed":19,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"}}},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"DL_colab_mp1.ipynb","provenance":[{"file_id":"1vsIaraCcTBle_HVpe5hg3EdoIlTyuETa","timestamp":1649842202836}],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}